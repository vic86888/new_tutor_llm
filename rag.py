import os, json, requests
import shutil
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain.schema import Document
from langchain_chroma import Chroma
import prompt

# ğŸ“š Step 1: åˆ‡ç‰‡
def load_and_chunk(filepath, chunk_size=1000, overlap=100):
    loader = TextLoader(filepath, encoding="utf-8")
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return splitter.split_documents(docs)

# âš™ï¸ Step 2: ç”¢ç”Ÿ embeddings
class GitHubEmbeddings:
    def __init__(self, model="openai/text-embedding-3-small"):
        load_dotenv()
        self.model = model
        self.token = os.getenv("GITHUB_TOKEN")
        self.url = "https://models.github.ai/inference/embeddings"
        self.headers = {
            "Authorization": f"Bearer {self.token}",
            "Accept": "application/vnd.github+json",
            "Content-Type": "application/json",
        }

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        resp = requests.post(self.url, headers=self.headers, json={"model": self.model, "input": texts})
        resp.raise_for_status()
        return [item["embedding"] for item in resp.json()["data"]]

    def embed_query(self, text: str) -> list[float]:
        return self.embed_documents([text])[0]

# ğŸ—„ Step 3: å»ºç«‹å‘é‡åº«
def build_vector_store(chunks):
    emb = GitHubEmbeddings()
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=emb,
        persist_directory="chroma_db"
    )
    # vectordb.persist()
    return vectordb

# ğŸ’¬ Step 4: ä½¿ç”¨ Chat æ¨¡å‹æ•´åˆæª¢ç´¢çµæœ
def chat_with_context(chunks, user_input, messages):
    messages.append({"role": "user", "content": user_input})

    # å‘¼å« Chat æ¥å£
    load_dotenv()
    token = os.getenv("GITHUB_TOKEN")
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github+json",
        "Content-Type": "application/json"
    }
    url = "https://models.github.ai/inference/chat/completions"

    resp = requests.post(url, headers=headers, json={
        "model": "openai/gpt-4.1-mini",
        "messages": messages
    })
    resp.raise_for_status()

    reply = resp.json()["choices"][0]["message"]["content"]
    messages.append({"role": "assistant", "content": reply})
    return reply

# â–¶ï¸ ä¸»æµç¨‹
if __name__ == "__main__":
    # æ¸…é™¤èˆŠçš„å‘é‡åº«
    if os.path.exists("chroma_db"):
        shutil.rmtree("chroma_db")

    messages = [{"role": "system", "content": prompt.tutor_guideline}]
    chunks = load_and_chunk(r"C:\gpt_tutor\py4e_3.txt")
    print(f"å·²åˆ‡å‡º {len(chunks)} ç‰‡")

    vectordb = build_vector_store(chunks)
    print("âœ”ï¸ å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆ")

    print("è¼¸å…¥å•é¡Œï¼Œè¼¸å…¥ exit é›¢é–‹ã€‚")

    # æŠŠæ•™ææ‘˜è¦å‡ºä¾†ï¼ˆç”¨ similarity_search æŠ“å¹¾æ®µå…§å®¹ï¼‰
    results = vectordb.similarity_search("Python", k=3)
    context = "\n\n---\n\n".join([doc.page_content for doc in results])

    # æŠŠæ•™æå…§å®¹æ”¾é€²é–‹å ´è¨Šæ¯ä¸­
    intro_question = f"""
    ç•¶ä½ æ”¶åˆ°æ•™æå¾Œï¼Œè«‹å…ˆé–±è®€æ•™æå…§å®¹ï¼Œæ­¸ç´å‡º 2ï½5 å€‹å­¸ç”Ÿä»Šå¤©è¦å­¸ç¿’çš„é‡é»ï¼Œä¸¦ç”¨ç°¡å–®æ¸…æ¥šçš„è©±åœ¨é–‹å ´æ™‚å‘Šè¨´å­¸ç”Ÿï¼š
    ã€Œæˆ‘å€‘ä»Šå¤©æœƒå­¸åˆ°ä»€éº¼ã€ã€‚è«‹åˆ—é»æˆ–æ¢åˆ—æ–¹å¼å‘ˆç¾ï¼Œå¹«åŠ©å­¸ç”Ÿå»ºç«‹å­¸ç¿’æœŸå¾…ã€‚
    ç„¶å¾Œå’Œå­¸ç”Ÿç¢ºèªæ˜¯å¦æº–å‚™é–‹å§‹å­¸ç¿’

    æ•™æå…§å®¹ï¼š
    {context}
    """

    first_question = chat_with_context(vectordb, intro_question, messages)
    print("\nåŠ©ç†ï¼š", first_question)

    while True:
        q = input("\nä½ ï¼š").strip()
        if q.lower() in ("exit","quit"):
            print("ğŸ‘‹ å†è¦‹ï¼"); break
        answer = chat_with_context(vectordb, q, messages)
        print("\nåŠ©ç†ï¼š", answer)
