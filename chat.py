# chat.py
# -*- coding: utf-8 -*-
import yaml
from sentence_transformers import CrossEncoder # ç”¨æ–¼é‡æ’æ¨¡å‹
from langchain_community.chat_message_histories import ChatMessageHistory
from tutor_agent import TutorAgent
from vector_store import reset_db  # è‹¥å¾ŒçºŒéœ€è¦é‡ç½®
import verify  # æ–°å¢ï¼šé©—è­‰æ¨¡çµ„

# é¸ä¸€å€‹åˆé©çš„é‡æ’æ¨¡å‹
# ms-marco-MiniLM-L-6-v2 åœ¨é€Ÿåº¦èˆ‡æ•ˆæœé–“æœ‰ä¸éŒ¯çš„å¹³è¡¡
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# è¼‰å…¥è¨­å®š
CFG = yaml.safe_load(open("config.yaml", encoding="utf-8"))

# åˆå§‹åŒ–å°è©±ä»£ç†
agent = TutorAgent()

# è¼‰å…¥å·²å­˜åœ¨çš„å‘é‡åº«ï¼ˆå‡è¨­ persist_directory åœ¨ config.yaml ä¸­ï¼‰
from langchain_chroma import Chroma
from embeddings import GitHubEmbeddings

emb = GitHubEmbeddings()
vector_dir = CFG["vector_db_dir"]

vectordb = Chroma(
    persist_directory=vector_dir,
    embedding_function=emb
)
print("æˆåŠŸè¼‰å…¥å‘é‡åº«ï¼Œé–‹å§‹å°è©±")
# åœ¨é€™è£¡æ–°å¢
history = ChatMessageHistory()
agent = TutorAgent()

# é–‹å ´æ‘˜è¦
# context = "\n\n---\n\n".join(
#     [doc.page_content for doc in vectordb.similarity_search("intro", k=CFG["top_k_intro"]) ]
# )
from prompt import tutor_guideline
# intro_prompt = f"""
# ç³»çµ±æç¤ºï¼š{tutor_guideline}

# è«‹ä½¿ç”¨è€…æå‡ºå’Œåƒè€ƒè³‡æ–™ç›¸é—œçš„å•é¡Œï¼š
# åƒè€ƒè³‡æ–™ï¼š
# {context}
# """
# print("\nåŠ©ç†ï¼š", agent.ask(intro_prompt))
print("\nåŠ©ç†ï¼š", "æ­¡è¿ä½¿ç”¨çŸ¥è­˜å•ç­”ç³»çµ±ï¼è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼Œæˆ‘æœƒæ ¹æ“šæä¾›çš„ç”¢å“è³‡è¨Šå›ç­”ã€‚")


# å•ç­”è¿´åœˆ
from main import multiline_input
while True:
    user_input = multiline_input()
    if user_input.lower() in {"exit", "quit", "bye"}:
        print("ğŸ‘‹ å†è¦‹ï¼")
        break
    # 1. æ ¹æ“šä½¿ç”¨è€…å•é¡Œåšç›¸ä¼¼åº¦æœå°‹
    relevant_docs = vectordb.similarity_search(
        user_input,
        k=CFG.get("top_k_query", 5)
    )
    context = "\n\n---\n\n".join([doc.page_content for doc in relevant_docs])

     # 1.5 åŠ å…¥é‡æ’ (reâ€‘ranking)
    #  1) æº–å‚™ (query, doc_text) pair list
    pairs = [
        (user_input, doc.page_content)
        for doc in relevant_docs
    ]
    #  2) ç”¨ CrossEncoder é æ¸¬æ¯ä¸€å°çš„ç›¸é—œåº¦
    scores = cross_encoder.predict(pairs)
    # 3. æŠŠ doc è·Ÿ score æ‰“åŒ…æˆ list of tuples
    doc_score_pairs = list(zip(relevant_docs, scores))

    # 4. ä¾ score æ’åºï¼ˆç”±å¤§åˆ°å°ï¼‰
    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

    # å–é‡æ’å¾Œåˆ†æ•¸æœ€é«˜çš„å‰ N å€‹ chunk
    top_k = CFG.get("top_k_rerank", 3)
    selected = doc_score_pairs[:top_k]

    # â€”â€” åœ¨é€™è£¡å°å‡ºæœ¬æ¬¡ç”¨åˆ°çš„ chunk â€”â€”  
    print("\næœ¬æ¬¡ä½¿ç”¨çš„ chunk (å·²ä¾ç›¸é—œåº¦æ’åº)ï¼š")
    for idx, (doc, score) in enumerate(selected, start=1):
        # å‡è¨­ä½ åœ¨å»ºç«‹å‘é‡åº«æ™‚æœ‰æŠŠ source è¨˜åœ¨ metadata è£¡
        source = doc.metadata.get("source", "unknown")
        snippet = doc.page_content.replace("\n", " ")[:100]  # å–å‰100å­—
        print(f"{idx}. [score={score:.3f}] ä¾†æº: {source}ï¼Œå…§å®¹é è¦½: {snippet}â€¦")

    # 5. ç”¢ç”Ÿæ’åºå¾Œçš„ contextï¼ˆå¯åŒæ™‚é¡¯ç¤ºåˆ†æ•¸ï¼‰
    context = "\n\n---\n\n".join(
        f"[score={score:.3f}] {doc.page_content}"
        for doc, score in selected
    )

    # 6. æŠŠæ’åºå¾Œçš„ documents æ‹†å›ä¾†ï¼Œå¦‚æœå¾ŒçºŒéœ€è¦å†æ“ä½œ
    sorted_docs = [doc for doc, _ in doc_score_pairs]

    # 2. å–ç”¨å°è©±è¨˜æ†¶
    history_msgs = history.messages
    history_text = "\n".join([f"{msg.type}: {msg.content}" for msg in history_msgs])

    prompt = f"""
        ### â¤ System
        {tutor_guideline}

        ### â¤ Memory
        {history_text}

        ### â¤ ä¸‹é¢æ˜¯æ’åºå¾Œçš„åƒè€ƒè³‡æ–™ï¼ˆæ¯æ®µåŒ…å« reranker çµ¦çš„ç›¸é—œæ€§åˆ†æ•¸èˆ‡ä¾†æºï¼‰ï¼Œè«‹åªæ ¹æ“šé€™äº›è³‡æ–™å›ç­”å•é¡Œï¼Œä¸è¦æ†‘ç©ºè£œå……æœªå‡ºç¾åœ¨è³‡æ–™ä¸­çš„å…§å®¹ã€‚å·®ç•°é«˜çš„åˆ†æ•¸ä»£è¡¨å…§å®¹æ¯”è¼ƒåŒ¹é…ï¼Œä½†æœ€çµ‚ç­”æ¡ˆæ‡‰ä»¥æ–‡å­—å…§å®¹ç‚ºä¾æ“šã€‚
        {context}

        ### â¤ User
        {user_input}
        """


    # 4. å‘¼å«æ¨¡å‹ä¸¦å°å‡ºå›ç­”
    answer = agent.ask(prompt)
    print("\nåŠ©ç†ï¼š", answer)

    # 5. æ›´æ–°å°è©±è¨˜æ†¶
    history.add_user_message(user_input)
    history.add_ai_message(answer)

    # 6. é©—è­‰éšæ®µ
    report = verify.verify_answer(user_input, answer, context)
    print("\nğŸ” é©—è­‰å ±å‘Šï¼š", report)
    
    # 1. å°‡å•é¡Œå‘é‡åŒ–ï¼Œä¸¦æ‰¾å‡ºå’Œå•é¡Œæœ€åŒ¹é…çš„æ–‡æœ¬ä¸€èµ·ä¸Ÿçµ¦èªè¨€æ¨¡å‹ï¼Œé€™æ˜¯æˆ‘ã€Œæå• èªè¨€æ¨¡å‹å›ç­”ã€åœ¨ä½¿ç”¨çš„æ–¹å¼
    
    # 2. æˆ‘å€‘ç›®å‰åšçš„å®¶æ•™æ˜¯ã€Œå’Œèªè¨€æ¨¡å‹å°è©±ã€ï¼Œå’Œç›®å‰è¦åšçš„ragæŠ€è¡“è¡çª

    # 3. ç›®å‰ç¨‹å¼åœ¨é–‹å ´æ™‚æœƒä»¥ "intro" ç‚ºæŸ¥è©¢ä¸€æ¬¡æ€§åœ°å¾å‘é‡åº«æ’ˆå– 2â€“5 å€‹é‡é»ç‰‡æ®µï¼Œä¹‹å¾Œæ‰€æœ‰çš„ä½¿ç”¨è€…æå•
    # ï¼ˆagent.ask(user_msg)ï¼‰ä¸¦æ²’æœ‰å†ç¶“éå‘é‡åŒ–æª¢ç´¢ï¼Œè€Œæ˜¯ç›´æ¥æŠŠæœ€æ–°çš„å°è©±æ­·å²ï¼ˆåŒ…å«ç³»çµ±æç¤ºèˆ‡ä¹‹å‰çš„å•ç­”ï¼‰
    # å‚³çµ¦æ¨¡å‹ï¼Œç„¡é¡å¤–æ‹‰å–æ–°çš„ä¸Šä¸‹æ–‡ã€‚

    # 4. ä¸Šæ–¹ç‚ºç›®å‰çš„å°è©±
    # ä¸‹æ–¹ç‚ºç”¨æˆ¶æ¯æ¬¡çš„å›ç­”éƒ½æœƒåœ¨ç¶“éå‘é‡åŒ–æª¢ç´¢å¾Œï¼Œå°‡ç›¸é—œç‰‡æ®µèˆ‡å•é¡Œä¸€èµ·å‚³çµ¦æ¨¡å‹ï¼Œ"ç„¡æ³•æŒçºŒé€²è¡Œå°è©±"

    # æ‰€ä»¥promptå¿…é ˆé‡æ–°è¨­è¨ˆï¼

    '''
        # 1. æ ¹æ“šä½¿ç”¨è€…è¼¸å…¥åšç›¸ä¼¼åº¦æœå°‹
        relevant = vectordb.similarity_search(user_input, k=CFG["top_k_query"])  # :contentReference[oaicite:5]{index=5}
        # 2. æ‹¼æ¥æœå°‹åˆ°çš„ç‰‡æ®µä½œç‚ºä¸Šä¸‹æ–‡
        context = "\n\n---\n\n".join(doc.page_content for doc in relevant)
        # 3. å»ºç«‹æ–°çš„ promptï¼ŒæŠŠä¸Šä¸‹æ–‡èˆ‡æå•ä¸€åŒå‚³çµ¦æ¨¡å‹
        qa_prompt = f"""
        ä»¥ä¸‹æ˜¯èˆ‡ä½ å•é¡Œæœ€ç›¸é—œçš„æ•™æç‰‡æ®µï¼Œè«‹åƒè€ƒå¾Œå†å›ç­”ï¼š

        {context}

        å•é¡Œï¼š{user_input}
        """
        # 4. å‘¼å«æ¨¡å‹
        print("\nåŠ©ç†ï¼š", agent.ask(qa_prompt))
    '''