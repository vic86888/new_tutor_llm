# chat_stream_rag.py
# -*- coding: utf-8 -*-
"""
RAG å°è©±ï¼šå‘é‡æª¢ç´¢(Chroma) +ï¼ˆå¯é¸ï¼‰BM25(FTS5) + å»é‡ + BGE Reranker â†’ Ollama ä¸²æµç”Ÿæˆ
æ¨¡å‹ï¼šcwchang/llama-3-taiwan-8b-instruct:latest
"""

import sys, os, json, requests, sqlite3, numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path

# ------- é–’èŠå¸¸æ•¸é–€æª» -------
RERANK_MIN = 0.2     # äº¤å‰ç·¨ç¢¼å™¨åˆ†æ•¸é–€æª»ï¼Œéä½è¦–ç‚ºä¸å¯é 
QUERY_LEN_MIN = 2    # å°‘æ–¼é€™å€‹é•·åº¦ï¼Œå¤šåŠç•¶é–’èŠ


# ------- å¯èª¿åƒæ•¸ -------
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://127.0.0.1:11434/api/chat")
LLM_MODEL  = os.getenv("OLLAMA_MODEL", "cwchang/llama-3-taiwan-8b-instruct:latest")

CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "chroma_db")
CHROMA_COLLECTION = os.getenv("CHROMA_COLLECTION", "campus_news_bgem3")
FTS_DB = "fts5_db/fts5.db"

DEVICE = os.getenv("RAG_DEVICE", "cuda")
EMB_BATCH = int(os.getenv("RAG_EMB_BATCH", "64"))
POOL_K_VEC = int(os.getenv("RAG_POOL_K_VEC", "50"))
POOL_K_BM25 = int(os.getenv("RAG_POOL_K_BM25", "50"))
TOP_K = int(os.getenv("RAG_TOP_K", "5"))

SHOW_SCORES = os.getenv("RAG_SHOW_SCORES", "0") != "0"   # 1=åœ¨ä¾†æºç¯€éŒ„é¡¯ç¤ºåˆ†æ•¸
SCORE_DIGITS = int(os.getenv("RAG_SCORE_DIGITS", "3"))    # å°æ•¸ä½æ•¸

RAG_USE_BM25 = os.getenv("RAG_USE_BM25", "0") != "0"  # 0=ä¸ç”¨ BM25ï¼ˆé è¨­ï¼‰ï¼Œ1=å•Ÿç”¨

# ------- ä¾è³´ï¼šå‘é‡æª¢ç´¢èˆ‡ rerank -------
import chromadb
from FlagEmbedding import BGEM3FlagModel, FlagReranker

TZ_TW = timezone(timedelta(hours=8))

# æ„åœ–åˆ¤åˆ¥ï¼šåªæ’é™¤æ˜é¡¯å¯’æš„ï¼›é è¨­å•Ÿç”¨ RAG
def is_info_query(q: str) -> bool:
    q = (q or "").strip()
    if not q:
        return False

    # æ˜é¡¯å¯’æš„ç›´æ¥ç•¶èŠå¤©
    smalltalk = {"ä½ å¥½","å—¨","å“ˆå›‰","åœ¨å—","è¬è¬","æ°æ°","æ™šå®‰","æ—©å®‰","hi","hello","bye","thanks"}
    if q.lower() in smalltalk or q in smalltalk:
        return False

    # æŠ€è¡“é—œéµè©ç™½åå–®ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
    tech_kws = {"rag","retrieval","å‘é‡","embedding","é‡æ’","rerank","bm25","chroma","bge","ollama","fts5","ç´¢å¼•"}
    if any(k in q.lower() for k in tech_kws):
        return True

    # æ ¡å‹™é—œéµè©ä¿ç•™ï¼ˆå‘½ä¸­å°±ä¸€å®šæª¢ç´¢ï¼‰
    campus_kws = {"ç”³è«‹","å ±å","æ™‚é–“","æ—¥æœŸ","æˆªæ­¢","æœŸé™","è³‡æ ¼","è¦å®š","ç¹³è²»","å­¸é›œè²»","æ¸›å…",
                  "ä½å®¿","é€€è²»","èª²ç¨‹","é¸èª²","åŠ é€€é¸","å…¬å‘Š","é€šçŸ¥","çå­¸é‡‘","ç«¶è³½","ä¸‹è¼‰","è¡¨å–®"}
    if any(k in q for k in campus_kws):
        return True

    # å…¶é¤˜é è¨­ä¹Ÿå•Ÿç”¨ RAG
    return True

def l2_normalize(vectors: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-12
    return vectors / norms

def embed_texts(model, texts, batch_size=64):
    out = model.encode(texts, batch_size=batch_size, return_dense=True,
                       return_sparse=False, return_colbert_vecs=False)
    dense = np.array(out["dense_vecs"], dtype=np.float32)
    return l2_normalize(dense)

def bm25_candidates(fts_db_path: str, query: str, k=50):

    import re
    # æ–¹æ³•ä¸€ï¼šå°‡ FTS5 ä¸å…è¨±çš„ç‰¹æ®Šå­—å…ƒæ›¿æ›æ‰
    safe_query = re.sub(r'[\"\'\?\*\(\):]', ' ', query)

    p = Path(fts_db_path)
    if not fts_db_path or not p.exists():
        return []
    con = sqlite3.connect(str(p))
    con.row_factory = sqlite3.Row
    rows = con.execute("""
      SELECT title, text, url, published_at, doc_id, chunk_id, bm25(chunks_fts) AS score
      FROM chunks_fts
      WHERE chunks_fts MATCH ?
      ORDER BY score LIMIT ?;
    """, (safe_query, k)).fetchall()
    con.close()
    out = []
    for r in rows:
        sim = 1.0 / (1.0 + float(r["score"]))  # bm25 åˆ†æ•¸è¶Šå°è¶Šå¥½ï¼Œè½‰æˆç›¸ä¼¼åº¦
        out.append({
            "id": r["chunk_id"],
            "doc": r["text"],
            "meta": {
                "title": r["title"], "url": r["url"],
                "published_at": r["published_at"], "doc_id": r["doc_id"]
            },
            "bm25_sim": sim,
        })
    return out

class Retriever:
    def __init__(self, db_path=CHROMA_DB_PATH, collection=CHROMA_COLLECTION,
                 fts_db=FTS_DB, device=DEVICE, emb_batch=EMB_BATCH):
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_collection(name=collection)
        self.embed_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True, device=device)
        self.reranker = FlagReranker("BAAI/bge-reranker-base", use_fp16=True, device=device)
        self.fts_db = fts_db
        self.emb_batch = emb_batch

    def vector_candidates(self, query: str, k=50):
        q_vec = embed_texts(self.embed_model, [query], batch_size=1).tolist()
        res = self.collection.query(
            query_embeddings=q_vec, n_results=k,
            include=["documents", "metadatas", "distances"]
        )
        ids  = res["ids"][0]
        docs = res["documents"][0]
        metas= res["metadatas"][0]
        dists= res.get("distances", [[]])[0] or [0.0]*len(docs)
        out = []
        for cid, doc, meta, dist in zip(ids, docs, metas, dists):
            if not (doc or "").strip(): 
                continue
            sim = 1.0 - float(dist)  # cosine è·é›¢ â†’ ç›¸ä¼¼åº¦
            out.append({"id": cid, "doc": doc, "meta": meta or {}, "vec_sim": sim})
        return out

    def hybrid_search(self, query: str, top_k=TOP_K, pool_k_vec=POOL_K_VEC, pool_k_bm25=POOL_K_BM25,
                  recent_days=None):
        # 1) å‘é‡å€™é¸
        vec = self.vector_candidates(query, k=pool_k_vec)

        # 2)ï¼ˆå¯é¸ï¼‰BM25 å€™é¸ï¼šé è¨­é—œé–‰
        if RAG_USE_BM25:
            bm25 = bm25_candidates(self.fts_db, query, k=pool_k_bm25)
        else:
            bm25 = []

        # 3) åˆä½µ
        by_id = {}
        for h in vec:
            key = h["id"]
            cur = by_id.get(key, {"id": key, "doc": h["doc"], "meta": h.get("meta", {}),
                                "vec_sim": 0.0, "bm25_sim": 0.0})
            cur["vec_sim"] = max(cur["vec_sim"], h.get("vec_sim", 0.0))
            by_id[key] = cur

        for h in bm25:
            key = h["id"]
            cur = by_id.get(key, {"id": key, "doc": h["doc"], "meta": h.get("meta", {}),
                                "vec_sim": 0.0, "bm25_sim": 0.0})
            cur["bm25_sim"] = max(cur["bm25_sim"], h.get("bm25_sim", 0.0))
            by_id[key] = cur

        merged = list(by_id.values())

        # 4)ï¼ˆå¯é¸ï¼‰æ™‚é–“éæ¿¾
        if recent_days:
            cutoff = datetime.now(TZ_TW) - timedelta(days=recent_days)
            def ok(meta):
                try:
                    dt = datetime.fromisoformat((meta.get("published_at") or "").replace("Z","+00:00"))
                    return dt >= cutoff
                except Exception:
                    return True
            merged = [x for x in merged if ok(x["meta"])]

        # 5) å»é‡ï¼ˆè£œ url ä½œç‚º fallbackï¼Œé¿å…åŒé æ´—ç‰ˆï¼‰
        def score_heur(x, a=0.6, b=0.4): return a*x["vec_sim"] + b*x["bm25_sim"]
        best_by_doc = {}
        for x in merged:
            m = x.get("meta") or {}
            key = m.get("doc_id") or m.get("checksum") or m.get("url") or x["id"]
            cur = best_by_doc.get(key)
            if cur is None or score_heur(x) > score_heur(cur):
                best_by_doc[key] = x
        uniq = list(best_by_doc.values())

        # 6) Rerankï¼ˆç”¨èªæ„å•å¥ï¼‰
        if not uniq:
            return []
        pairs = [[query, u["doc"]] for u in uniq]
        scores = self.reranker.compute_score(pairs, batch_size=64)
        for u, s in zip(uniq, scores):
            u["rr_score"] = float(s)
        uniq.sort(key=lambda x: x["rr_score"], reverse=True)
        return uniq[:top_k]

# ------- èˆ‡ Ollama ä¸²æµ -------
def check_server():
    try:
        r = requests.get("http://127.0.0.1:11434/api/version", timeout=2)
        r.raise_for_status()
        return True
    except Exception:
        print("âŒ ç„¡æ³•é€£åˆ° Ollamaï¼Œè«‹å…ˆåŸ·è¡Œï¼š`ollama serve`")
        return False

def build_context(hits):
    blocks = []
    for i, h in enumerate(hits, 1):
        m = h.get("meta", {}) or {}
        title = m.get("title", "(ç„¡æ¨™é¡Œ)")
        url   = m.get("url", "")
        pub   = m.get("published_at", "")
        snippet = (h.get("doc") or "").strip()
        blocks.append(f"[{i}] {title} | {pub}\n{url}\n{snippet}\n")
    return "\n".join(blocks)

def build_messages(user_query: str, ctx: str):
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½æ‡‚å°ç£åœ¨åœ°èªå¢ƒã€ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†çš„æ ¡å‹™åŠ©ç†ã€‚\n"
        "å¿…é ˆåªæ ¹æ“šæä¾›çš„ã€ä¾†æºç¯€éŒ„ã€å›ç­”ï¼›ä¸è¦è‡†æ¸¬ã€‚\n"
        "é‡è¦æ—¥æœŸ/æ•¸å­—è¦æ˜ç¢ºå¯«å‡ºï¼Œä¸¦åœ¨ç›¸é—œå¥å¾Œç”¨ä¾†æºç·¨è™Ÿæ¨™è¨»ï¼Œå¦‚ [1][2]ã€‚\n"
        "è‹¥ä¾†æºæ²’æœ‰ç­”æ¡ˆï¼Œè«‹æ˜ç¢ºèªªæ˜æ‰¾ä¸åˆ°ï¼Œä¸¦å»ºè­°æŸ¥çœ‹åŸæ–‡é€£çµã€‚"
    )
    user_payload = (
        f"å•é¡Œï¼š{user_query}\n\n"
        f"=== ä¾†æºç¯€éŒ„ï¼ˆè«‹å‹™å¿…å¼•ç”¨ç·¨è™Ÿï¼‰ ===\n{ctx}\n=== çµæŸ ===\n"
        f"è«‹ç”¨ä¸­æ–‡å›ç­”ï¼Œæœ€å¾Œåˆ—å‡ºåƒè€ƒä¾†æºç·¨è™Ÿèˆ‡é€£çµã€‚"
    )
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_payload},
    ]

def stream_chat(messages, temperature=0.2):
    payload = {
        "model": LLM_MODEL,
        "messages": messages,
        "stream": True,
        "options": {"temperature": temperature}
    }
    with requests.post(OLLAMA_URL, json=payload, stream=True) as r:
        r.raise_for_status()
        buf = []
        for line in r.iter_lines(decode_unicode=True):
            if not line:
                continue
            try:
                chunk = json.loads(line)
            except json.JSONDecodeError:
                continue
            if "message" in chunk and "content" in chunk["message"]:
                token = chunk["message"]["content"]
                buf.append(token)
                print(token, end="", flush=True)
            if chunk.get("done"):
                print()
                break
        return "".join(buf)

def main():
    if not check_server():
        sys.exit(1)

    # æº–å‚™æª¢ç´¢å™¨
    print("ğŸš€ åˆå§‹åŒ–æª¢ç´¢å™¨ï¼ˆChroma + BGE-M3 + Rerankerï¼‰...")
    retriever = Retriever(db_path=CHROMA_DB_PATH, collection=CHROMA_COLLECTION,
                          fts_db=FTS_DB, device=DEVICE, emb_batch=EMB_BATCH)
    print(f"ğŸ’¬ èˆ‡ {LLM_MODEL} å°è©±ï¼ˆè¼¸å…¥ exit é›¢é–‹ï¼‰")

    history = []  # è‹¥è¦å®Œæ•´å¤šè¼ªï¼Œå¯å°‡ messages.extend(history)ï¼›æ­¤è™•æ¯è¼ªä»¥æœ€æ–°æª¢ç´¢ç‚ºæº–
    try:
        while True:
            user = input("ä½ ï¼š").strip()
            if user.lower() == "exit":
                break

            # å…ˆæª¢ç´¢
            use_rag = is_info_query(user)
            hits = []

            if use_rag:
                hits = retriever.hybrid_search(
                    user, top_k=TOP_K, pool_k_vec=POOL_K_VEC, pool_k_bm25=POOL_K_BM25, recent_days=None
                )
            # é¡¯ç¤ºå‰å¹¾åå‘½ä¸­èˆ‡åˆ†æ•¸ï¼ˆæ–¹ä¾¿äººçœ‹ï¼‰
            if hits:
                def fmt(x): return f"{x:.{SCORE_DIGITS}f}" if isinstance(x, (int, float)) else "NA"
                print("Top RAG hits:")
                for i, h in enumerate(hits[:TOP_K], 1):
                    m = h.get("meta", {}) or {}
                    if RAG_USE_BM25:
                        print(f"  #{i} rr={fmt(h.get('rr_score'))} | vec={fmt(h.get('vec_sim'))} | bm25={fmt(h.get('bm25_sim'))} | {m.get('title')} | {m.get('url')}")
                    else:
                        print(f"  #{i} rr={fmt(h.get('rr_score'))} | vec={fmt(h.get('vec_sim'))} | {m.get('title')} | {m.get('url')}")

                # åˆ†æ•¸é–€æª»ï¼šè‹¥æœ€ç›¸é—œä¹Ÿå¤ªä½ï¼Œå°±è¦–åŒä¸æª¢ç´¢
                if not hits or (hits and hits[0].get("rr_score", 0.0) < RERANK_MIN):
                    use_rag = False

            if not use_rag:
                # ç´”èŠå¤©
                messages = [{"role":"user","content": user}]
            else:
                ctx = build_context(hits)
                messages = build_messages(user, ctx)

            print("æ¨¡å‹ï¼š", end="", flush=True)
            reply = stream_chat(messages, temperature=0.2)
            # å¦‚éœ€å¤šè¼ªä¸Šä¸‹æ–‡å¯ push é€² historyï¼ˆæ³¨æ„é•·åº¦ï¼‰
            # history += [{"role":"user","content":user},{"role":"assistant","content":reply}]
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²ä¸­æ­¢")

if __name__ == "__main__":
    main()

